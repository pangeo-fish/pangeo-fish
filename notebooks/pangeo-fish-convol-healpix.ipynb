{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# **Example Usage of Pangeo-Fish Software with Healpix Convolution**\n",
    "\n",
    "\n",
    "**Overview:**\n",
    "This Jupyter notebook demonstrates the usage of the Pangeo-Fish software, a tool designed for analyzing biologging data in reference to Earth Observation (EO) data. Specifically, it utilizes data employed in the study conducted by M. Gonze et al. titled \"Combining acoustic telemetry with archival tagging to investigate the spatial dynamics of the understudied pollack *Pollachius pollachius*,\" accepted for publication in the Journal of Fish Biology.\n",
    "\n",
    "We showcase the application using the biologging tag 'A19124' attached to a pollack fish, along with reference EO data from the European Union Copernicus Marine Service Information (CMEMS) product 'NORTHWESTSHELF_ANALYSIS_FORECAST_PHY_004_013'. The biologging data consist of Data Storage Tag (DST) and teledetection by acoustic signals, along with release and recapture time and location of the species in question.  Both biologging data and the reference EO data are accessible with https and the access methods are incropolated in this notebook.   \n",
    "\n",
    "\n",
    "\n",
    "**Purpose:**\n",
    "By executing this notebook, users will learn how to set up a workflow for utilizing the Pangeo-Fish software. The workflow consists of 9 steps which are described below:\n",
    "\n",
    "1. **Configure the Notebook:** Prepare the notebook environment for analysis.\n",
    "2. **Compare Reference Model with DST Information:** Analyze and compare data from the reference model with information from the biologging data of the species in question. \n",
    "3. **Regrid the Grid from Reference Model Grid to Healpix Grid:** Transform the grid from the reference model to the Healpix grid for further analysis.\n",
    "4. **Construct Emission Matrix:** Create an emission matrix based on the transformed grid.\n",
    "5. **Compute Additional Emission Probability Matrix:** Calculate an additional emission probability matrix, particularly focusing on teledetection from acoustic signals.\n",
    "6. **Combine and Normalize Emission Matrix:** Merge the emission matrix and normalize it for further processing.\n",
    "7. **Estimate Model Parameters:** Determine the parameters of the model based on the normalized emission matrix.\n",
    "8. **Compute State Probabilities and Tracks:** Calculate the probability distribution of the species in question and compute the tracks.\n",
    "9. **Visualization:** Visualize the results of the analysis for interpretation and insight.\n",
    "\n",
    "Throughout this notebook, users will gain practical experience in setting up and executing a workflow using Pangeo-Fish, enabling them to apply similar methodologies to their own biologging data analysis tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 1. **Configure the Notebook:** Prepare the notebook environment for analysis.\n",
    "\n",
    "In this step, we sets up the notebook environment for analysis. It includes installing necessary packages, importing required libraries, setting up parameters, and configuring the cluster for distributed computing. It also retrieves the tag data needed for analysis.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "2",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Run the following 3 lines of command to install pangeo-fish in the pangeo environment.\n",
    "# Note: These commands install required packages for the analysis.\n",
    "# You may need to restart your kernel before executing the next cell.\n",
    "#\n",
    "!pip install rich zstandard xmovie\n",
    "!pip install \"xarray-healpy @ git+https://github.com/iaocea/xarray-regridding.git@0ffca6058f4008f4f22f076e2d60787fcf32ac82\"\n",
    "!pip install -e ../."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries and modules.\n",
    "import xarray as xr\n",
    "from pint_xarray import unit_registry as ureg\n",
    "from pangeo_fish.io import open_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Set up execution parameters for the analysis.\n",
    "#\n",
    "# Note: This cell is tagged as parameters, allowing automatic updates when configuring with papermil.\n",
    "\n",
    "# tag_name corresponds to the name of the biologging tag name (DST identification number),\n",
    "# which is also a path for storing all the information for the specific fish tagged with tag_name.\n",
    "tag_name = \"A19124\"\n",
    "\n",
    "# tag_root specifies the root URL for tag data used for this computation.\n",
    "tag_root = \"https://data-taos.ifremer.fr/data_tmp/cleaned/tag/\"\n",
    "\n",
    "# catalog_url specifies the URL for the catalog for reference data used.\n",
    "catalog_url = \"https://data-taos.ifremer.fr/kerchunk/ref-copernicus.yaml\"\n",
    "open_catalog = True\n",
    "\n",
    "\n",
    "# scratch_root specifies the root directory for storing output files.\n",
    "scratch_root = \"s3://destine-gfts-data-lake/demo\"\n",
    "\n",
    "# storage_options specifies options for the filesystem storing output files.\n",
    "storage_options = {\n",
    "    \"anon\": False,\n",
    "    \"profile\": \"gfts\",\n",
    "    \"client_kwargs\": {\n",
    "        \"endpoint_url\": \"https://s3.gra.perf.cloud.ovh.net\",\n",
    "        \"region_name\": \"gra\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# if you are using local file system, activate following two lines\n",
    "scratch_root = \".\"\n",
    "storage_options = None\n",
    "\n",
    "# Default chunk value for time dimension.  This values depends on the configuration of your dask cluster.\n",
    "chunk_time = 24\n",
    "\n",
    "#\n",
    "# Parameters for step 2. **Compare Reference Model with DST Information:**\n",
    "#\n",
    "# bbox, bounding box, defines the latitude and longitude range for the analysis area.\n",
    "bbox = {\"latitude\": [46, 51], \"longitude\": [-8, -1]}\n",
    "\n",
    "# relative_depth_threshold defines the acceptable fish depth relative to the maximum tag depth.\n",
    "# It determines whether the fish can be considered to be in a certain location based on depth.\n",
    "relative_depth_threshold = 0.8\n",
    "\n",
    "#\n",
    "# Parameters for step 3. **Regrid the Grid from Reference Model Grid to Healpix Grid:**\n",
    "#\n",
    "# nside defines the resolution of the healpix grid used for regridding.\n",
    "nside = 4096  # *2\n",
    "\n",
    "# min_vertices sets the minimum number of vertices for a valid transcription for regridding.\n",
    "min_vertices = 1\n",
    "\n",
    "#\n",
    "# Parameters for step 4. **Construct Emission Matrix:**\n",
    "#\n",
    "# differences_std sets the standard deviation for scipy.stats.norm.pdf.\n",
    "# It expresses the estimated certainty of the field of difference.\n",
    "differences_std = 0.75\n",
    "\n",
    "# recapture_std sets the covariance for recapture event.\n",
    "# It shows the certainty of the final recapture area if it is known.\n",
    "recapture_std = 1e-2\n",
    "\n",
    "# earth_radius defines the radius of the Earth used for distance calculations.\n",
    "earth_radius = ureg.Quantity(6371, \"km\")\n",
    "\n",
    "# maximum_speed sets the maximum allowable speed for the tagged fish.\n",
    "maximum_speed = ureg.Quantity(60, \"km / day\")\n",
    "\n",
    "# adjustment_factor adjusts parameters for a more fuzzy search.\n",
    "# It will factor the allowed maximum displacement of the fish.\n",
    "adjustment_factor = 5\n",
    "\n",
    "# truncate sets the truncating factor for computed maximum allowed sigma for convolution process.\n",
    "truncate = 4\n",
    "\n",
    "#\n",
    "# Parameters for step 5. **Compute Additional Emission Probability Matrix:**\n",
    "#\n",
    "# receiver_buffer sets the maximum allowed detection distance for acoustic receivers.\n",
    "receiver_buffer = ureg.Quantity(1000, \"m\")\n",
    "\n",
    "#\n",
    "# Parameters for step 7. **Estimate Model Parameters:**\n",
    "#\n",
    "# tolerance sets the tolerance level for optimised parameter serarch computation.\n",
    "tolerance = 1e-6\n",
    "\n",
    "#\n",
    "# Parameters for step 8. **Compute State Probabilities and Tracks:**\n",
    "#\n",
    "# track_modes defines the modes for track calculation.\n",
    "track_modes = [\"mean\", \"mode\"]\n",
    "# track_modes = [\"mean\", \"mode\",\"mode_minrk\",\"mean_minrk\"]\n",
    "\n",
    "# additional_track_quantities sets quantities to compute for tracks using moving pandas.\n",
    "additional_track_quantities = [\"speed\", \"distance\"]\n",
    "\n",
    "\n",
    "#\n",
    "# Parameters for step 9. **Visualization:**\n",
    "#\n",
    "# time_step defines for each time_step value we make movie of state and emission matrix\n",
    "time_step = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define target root directories for storing analysis results.\n",
    "target_root = f\"{scratch_root}/{tag_name}\"\n",
    "\n",
    "# Defines default chunk size for optimisation.\n",
    "default_chunk = {\"time\": chunk_time, \"lat\": -1, \"lon\": -1}\n",
    "default_chunk_xy = {\"time\": chunk_time, \"x\": -1, \"y\": -1}\n",
    "default_chunk_cells = {\"time\": chunk_time, \"cells\": -1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up a local cluster for distributed computing.\n",
    "from distributed import LocalCluster\n",
    "\n",
    "cluster = LocalCluster()\n",
    "client = cluster.get_client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tag_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open and retrieve the tag data required for the analysis\n",
    "tag = open_tag(tag_root, tag_name)\n",
    "tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 2. **Compare Reference Model with DST Tag Information:** Analyze and compare data from the reference model with information from the biologging data of the species in question. \n",
    "\n",
    "In this step, we compare the reference model data with Data Storage Tag information.\n",
    "The process involves reading and cleaning the reference model, aligning time, converting depth units, subtracting tag data from the model, and saving the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import intake\n",
    "from pangeo_fish.cf import bounds_to_bins\n",
    "from pangeo_fish.diff import diff_z\n",
    "from pangeo_fish.io import open_copernicus_catalog\n",
    "from pangeo_fish.tags import adapt_model_time, reshape_by_bins, to_time_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop tag data outside the tagged events interval\n",
    "time_slice = to_time_slice(tag[\"tagging_events/time\"])\n",
    "tag_log = tag[\"dst\"].ds.sel(time=time_slice)\n",
    "\n",
    "# Verify the data\n",
    "import hvplot.xarray\n",
    "import cmocean\n",
    "from pangeo_fish.io import save_html_hvplot\n",
    "\n",
    "plot = (\n",
    "    (-tag[\"dst\"].pressure).hvplot(width=1000, height=500, color=\"blue\")\n",
    "    * (-tag_log).hvplot.scatter(\n",
    "        x=\"time\", y=\"pressure\", color=\"red\", size=5, width=1000, height=500\n",
    "    )\n",
    "    * (\n",
    "        (tag[\"dst\"].temperature).hvplot(width=1000, height=500, color=\"blue\")\n",
    "        * (tag_log).hvplot.scatter(\n",
    "            x=\"time\", y=\"temperature\", color=\"red\", size=5, width=1000, height=500\n",
    "        )\n",
    "    )\n",
    ")\n",
    "filepath = f\"{target_root}/tags.html\"\n",
    "\n",
    "save_html_hvplot(plot, filepath, storage_options)\n",
    "\n",
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open and clean reference model\n",
    "if open_catalog:\n",
    "    cat = intake.open_catalog(catalog_url)\n",
    "    model = open_copernicus_catalog(cat)\n",
    "else:\n",
    "    from pangeo_fish.io import open_copernicus_zarr\n",
    "\n",
    "    model = open_copernicus_zarr(\n",
    "        # model='GLOBAL_ANALYSISFORECAST_PHY_001_024',\n",
    "        # freq=\"D\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Subset the reference_model by\n",
    "# - align model time with the time of tag_log, also\n",
    "# - drop data for depth later that are unlikely due to the observed pressure from tag_log\n",
    "# - defined latitude and longitude of bbox.\n",
    "#\n",
    "reference_model = (\n",
    "    model.sel(time=adapt_model_time(time_slice))\n",
    "    .sel(lat=slice(*bbox[\"latitude\"]), lon=slice(*bbox[\"longitude\"]))\n",
    "    .pipe(\n",
    "        lambda ds: ds.sel(\n",
    "            depth=slice(None, (tag_log[\"pressure\"].max() - ds[\"XE\"].min()).compute())\n",
    "        )\n",
    "    )\n",
    ").chunk({\"time\": chunk_time, \"lat\": -1, \"lon\": -1, \"depth\": -1})\n",
    "reference_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Reshape the tag log, so that it bins to the time step of reference_model\n",
    "reshaped_tag = reshape_by_bins(\n",
    "    tag_log,\n",
    "    dim=\"time\",\n",
    "    bins=(\n",
    "        reference_model.cf.add_bounds([\"time\"], output_dim=\"bounds\")\n",
    "        .pipe(bounds_to_bins, bounds_dim=\"bounds\")\n",
    "        .get(\"time_bins\")\n",
    "    ),\n",
    "    bin_dim=\"bincount\",\n",
    "    other_dim=\"obs\",\n",
    ").chunk({\"time\": chunk_time})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Subtract the time_bined tag_log from the reference_model.\n",
    "# Here, for each time_bin, each observed value are compared with the correspoindng depth of reference_model using diff_z function.\n",
    "#\n",
    "diff = (\n",
    "    diff_z(reference_model, reshaped_tag, depth_threshold=relative_depth_threshold)\n",
    "    .assign_attrs({\"tag_id\": tag_name})\n",
    "    .assign(\n",
    "        {\n",
    "            \"H0\": reference_model[\"H0\"],\n",
    "            \"ocean_mask\": reference_model[\"H0\"].notnull(),\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "# Persist the diff data\n",
    "diff = diff.chunk(default_chunk).persist()\n",
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Verify the data\n",
    "diff[\"diff\"].count([\"lat\", \"lon\"]).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Save snapshot to disk\n",
    "diff.to_zarr(f\"{target_root}/diff.zarr\", mode=\"w\", storage_options=storage_options)\n",
    "\n",
    "# Cleanup\n",
    "del tag_log, cat, model, reference_model, reshaped_tag, diff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 3. **Regrid the Grid from Reference Model Grid to Healpix Grid:** Transform the grid from the reference model to the Healpix grid for further analysis.\n",
    "\n",
    "In this step, we regrid the data from the reference model grid to a Healpix grid. This process involves defining the Healpix grid, creating the target grid, computing interpolation weights, performing the regridding, and saving the regridded data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from xarray_healpy import HealpyGridInfo, HealpyRegridder\n",
    "from pangeo_fish.grid import center_longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Open the diff data and performs cleaning operations to prepare it for regridding.\n",
    "\n",
    "ds = (\n",
    "    xr.open_dataset(\n",
    "        f\"{target_root}/diff.zarr\",\n",
    "        engine=\"zarr\",\n",
    "        chunks={},\n",
    "        storage_options=storage_options,\n",
    "    )\n",
    "    .pipe(lambda ds: ds.merge(ds[[\"latitude\", \"longitude\"]].compute()))\n",
    "    .swap_dims({\"lat\": \"yi\", \"lon\": \"xi\"})\n",
    "    .drop_vars([\"lat\", \"lon\"])\n",
    ")\n",
    "# Trouver les valeurs minimales et maximales en ignorant les NaN\n",
    "min_diff = ds[\"diff\"].min(skipna=True).compute()\n",
    "max_diff = ds[\"diff\"].max(skipna=True).compute()\n",
    "\n",
    "print(f\"Valeur minimale de diff (en ignorant NaN) : {min_diff}\")\n",
    "print(f\"Valeur maximale de diff (en ignorant NaN) : {max_diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Define the target Healpix grid information\n",
    "grid = HealpyGridInfo(level=int(np.log2(nside)))\n",
    "target_grid = grid.target_grid(ds).pipe(center_longitude, 0)\n",
    "target_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Compute the interpolation weights for regridding the diff data\n",
    "regridder = HealpyRegridder(\n",
    "    ds[[\"longitude\", \"latitude\", \"ocean_mask\"]],\n",
    "    target_grid,\n",
    "    method=\"bilinear\",\n",
    "    interpolation_kwargs={\"mask\": \"ocean_mask\", \"min_vertices\": min_vertices},\n",
    ")\n",
    "regridder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Perform the regridding operation using the computed interpolation weights.\n",
    "regridded = regridder.regrid_ds(ds).assign_coords(\n",
    "    cell_ids=lambda ds: ds.cell_ids.astype(\"int64\")\n",
    ")\n",
    "regridded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This cell verifies the regridded data by plotting the count of non-NaN values.\n",
    "regridded[\"diff\"].count([\"cells\"]).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "regridded.to_zarr(\n",
    "    f\"{target_root}/diff-regridded-1D.zarr\",\n",
    "    mode=\"w\",\n",
    "    consolidated=True,\n",
    "    compute=True,\n",
    "    storage_options=storage_options,\n",
    ")\n",
    "\n",
    "# Cleanup unnecessary variables to free up memory\n",
    "del ds, grid, target_grid, regridder, regridded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 4. **Construct Emission Matrix:** Create an emission matrix based on the transformed 1D grid.\n",
    "\n",
    "In this step, we construct the emission probability matrix based on the differences between the observed tag temperature and the reference sea temperature computed in Workflow 2 and regridded in Workflow 3. The emission probability matrix represents the likelihood of observing a specific temperature difference given the model parameters and configurations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from toolz.dicttoolz import valfilter\n",
    "from pangeo_fish.distributions import create_covariances\n",
    "from pangeo_fish.distributions.healpix import normal_at\n",
    "from pangeo_fish.pdf import normal\n",
    "import xdggs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Open the regridded diff data\n",
    "differences = xr.open_dataset(\n",
    "    f\"{target_root}/diff-regridded-1D.zarr\",\n",
    "    engine=\"zarr\",\n",
    "    chunks={},\n",
    "    storage_options=storage_options,\n",
    ").pipe(lambda ds: ds.merge(ds[[\"latitude\", \"longitude\"]].compute()))\n",
    "differences\n",
    "# Set required attributes cleanly\n",
    "differences[\"cell_ids\"].attrs[\"grid_name\"] = \"healpix\"\n",
    "# get existing attrs\n",
    "attrs_to_keep = [\"level\", \"grid_name\"]\n",
    "# keep only specified attrs\n",
    "differences[\"cell_ids\"].attrs = {\n",
    "    key: value\n",
    "    for key, value in differences[\"cell_ids\"].attrs.items()\n",
    "    if key in attrs_to_keep\n",
    "}\n",
    "\n",
    "differences = differences.pipe(xdggs.decode)\n",
    "differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Compute initial and final position\n",
    "grid = differences[[\"latitude\", \"longitude\"]].compute()\n",
    "# print(grid['cell_ids'])\n",
    "initial_position = tag[\"tagging_events\"].ds.sel(event_name=\"release\")\n",
    "# cov = create_covariances(1e-6, coord_names=[\"latitude\", \"longitude\"])\n",
    "# sigma 1e-3**2 in the calculation\n",
    "initial_probability = normal_at(grid, pos=initial_position, sigma=1e-3)\n",
    "\n",
    "final_position = tag[\"tagging_events\"].ds.sel(event_name=\"fish_death\")\n",
    "if final_position[[\"longitude\", \"latitude\"]].to_dataarray().isnull().all():\n",
    "    final_probability = None\n",
    "else:\n",
    "    final_probability = normal_at(grid, pos=final_position, sigma=recapture_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid\n",
    "print(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# compute emission probability matrix\n",
    "\n",
    "emission_pdf = (\n",
    "    normal(differences[\"diff\"], mean=0, std=differences_std, dims=[\"cells\"])\n",
    "    .to_dataset(name=\"pdf\")\n",
    "    .assign(\n",
    "        valfilter(\n",
    "            lambda x: x is not None,\n",
    "            {\n",
    "                \"initial\": initial_probability,\n",
    "                \"final\": final_probability,\n",
    "                \"mask\": differences[\"ocean_mask\"],\n",
    "            },\n",
    "        )\n",
    "    )\n",
    "    .assign_attrs(differences.attrs)  # | {\"max_sigma\": max_sigma})\n",
    ")\n",
    "\n",
    "emission_pdf = emission_pdf.chunk(default_chunk_cells).persist()\n",
    "emission_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Verify the data\n",
    "emission_pdf[\"pdf\"].count([\"cells\"]).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This cell saves the emission data to Zarr format, then cleans up unnecessary variables to free up memory.\n",
    "\n",
    "emission_pdf.to_zarr(\n",
    "    f\"{target_root}/emission_1D.zarr\",\n",
    "    mode=\"w\",\n",
    "    consolidated=True,\n",
    "    storage_options=storage_options,\n",
    ")\n",
    "\n",
    "\n",
    "del differences, grid, initial_probability, final_probability, emission_pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 5. **Compute Additional Emission Probability Matrix:** Calculate an additional emission probability matrix, particularly focusing on teledetection from acoustic signals.\n",
    "\n",
    "In this step, we compute additional emission probabilities based on acoustic detections for the selected tag. These additional probabilities enhance the emission probability matrix constructed in step 4 by incorporating information from acoustic telemetry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Import necessary libraries and open data and perform initial setup\n",
    "from pangeo_fish import acoustic, utils\n",
    "import hvplot.xarray\n",
    "\n",
    "emission = xr.open_dataset(\n",
    "    f\"{target_root}/emission_1D.zarr\",\n",
    "    engine=\"zarr\",\n",
    "    chunks={},  # \"x\": -1, \"y\": -1},\n",
    "    storage_options=storage_options,\n",
    ")\n",
    "emission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Construct the emission probabilities based on acoustic detections\n",
    "# emission.cell_ids.lat=0\n",
    "emission.cell_ids.attrs[\"lat\"] = 0\n",
    "emission.cell_ids.attrs[\"lon\"] = 0\n",
    "\n",
    "acoustic_pdf = acoustic.emission_probability(\n",
    "    tag,\n",
    "    emission[[\"time\", \"cell_ids\", \"mask\"]].compute(),\n",
    "    receiver_buffer,\n",
    "    nondetections=\"mask\",\n",
    "    chunk_time=chunk_time,\n",
    "    cell_ids=\"keep\",\n",
    "    dims=[\"cells\"],\n",
    ")\n",
    "\n",
    "acoustic_pdf = acoustic_pdf.persist()\n",
    "print(acoustic_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Verify the data and visualize the acoustic detections\n",
    "tag[\"acoustic\"][\"deployment_id\"].hvplot.scatter(c=\"red\", marker=\"x\") * (\n",
    "    acoustic_pdf[\"acoustic\"] != 0\n",
    ").sum(dim=(\"cells\")).hvplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "acoustic_pdf[\"acoustic\"].count(dim=(\"cells\")).hvplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge and save the combined emission probability matrix with acoustic probabilities\n",
    "\n",
    "combined = emission.merge(acoustic_pdf)\n",
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This cell saves the emission data to Zarr format, then cleans up unnecessary variables to free up memory.\n",
    "\n",
    "combined.to_zarr(\n",
    "    f\"{target_root}/emission_1D_acoustic.zarr\",\n",
    "    mode=\"w\",\n",
    "    consolidated=True,\n",
    "    storage_options=storage_options,\n",
    ")\n",
    "# cleanup\n",
    "\n",
    "del emission, acoustic_pdf, combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 6. **Combine and Normalize Emission Matrix:** Merge the emission matrix and normalize it for further processing.\n",
    "\n",
    "In this step, we combine the emission probability matrix constructed in Workflow 4 and 5 then normalize it to ensure that the probabilities sum up to one. This step prepares the combined emission matrix for further analysis and interpretation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pangeo_fish.pdf import combine_emission_pdf\n",
    "import hvplot.xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open and combine the emission probability matrix\n",
    "\n",
    "combined = (\n",
    "    xr.open_dataset(\n",
    "        f\"{target_root}/emission_1D_acoustic.zarr\",\n",
    "        engine=\"zarr\",\n",
    "        chunks=default_chunk_cells,\n",
    "        inline_array=True,\n",
    "        storage_options=storage_options,\n",
    "    )\n",
    "    .pipe(combine_emission_pdf)\n",
    "    .chunk(default_chunk_cells)\n",
    "    .persist()  # convert to comment if the emission matrix does *not* fit in memory\n",
    ")\n",
    "combined\n",
    "print(combined.pdf.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Verify the data and visualize the sum of probabilities\n",
    "combined[\"pdf\"].sum([\"cells\"]).hvplot(width=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the combined and normalized emission matrix\n",
    "combined.to_zarr(\n",
    "    f\"{target_root}/combined_1D.zarr\",\n",
    "    mode=\"w\",\n",
    "    consolidated=True,\n",
    "    storage_options=storage_options,\n",
    ")\n",
    "del combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 7. **Estimate Model Parameters:** Determine the parameters of the model based on the normalized emission matrix.\n",
    "\n",
    "This step first estimates maxixmum allowed value of  model parameter 'sigma' max_sigma.  Then we\n",
    "create an optimizer with an expected parameter range, fitting the model to the normalized emission matrix.  \n",
    "The resulting optimized parameters is saved to a json file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries and modules for data analysis.\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from pangeo_fish.hmm.estimator import EagerEstimator\n",
    "from pangeo_fish.hmm.optimize import EagerBoundsSearch\n",
    "from pangeo_fish.utils import temporal_resolution\n",
    "from pangeo_fish.hmm.estimator import EagerEstimator\n",
    "from pangeo_fish.hmm.prediction import Gaussian1DHealpix\n",
    "from tlz.functoolz import curry\n",
    "\n",
    "# Open the data\n",
    "emission = xr.open_dataset(\n",
    "    f\"{target_root}/combined_1D.zarr\",\n",
    "    engine=\"zarr\",\n",
    "    chunks={},\n",
    "    inline_array=True,\n",
    "    storage_options=storage_options,\n",
    ")\n",
    "emission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute maximum displacement for each reference model time step\n",
    "# and estimate maximum sigma value for limiting the optimisation step\n",
    "\n",
    "earth_radius_ = xr.DataArray(earth_radius, dims=None)\n",
    "\n",
    "timedelta = temporal_resolution(emission[\"time\"]).pint.quantify().pint.to(\"h\")\n",
    "grid_resolution = earth_radius_ * emission[\"resolution\"].pint.quantify()\n",
    "\n",
    "maximum_speed_ = xr.DataArray(maximum_speed, dims=None).pint.to(\"km / h\")\n",
    "max_grid_displacement = maximum_speed_ * timedelta * adjustment_factor / earth_radius_\n",
    "max_sigma = max_grid_displacement.pint.to(\"dimensionless\").pint.magnitude / truncate\n",
    "emission.attrs[\"max_sigma\"] = max_sigma.item()\n",
    "max_sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Create and configure estimator and optimizer\n",
    "import xdggs\n",
    "\n",
    "emission = (\n",
    "    emission.compute()\n",
    ")  # Convert to comment if the emission matrix does *not* fit in memory\n",
    "\n",
    "# Set required attributes cleanly\n",
    "emission[\"cell_ids\"].attrs[\"grid_name\"] = \"healpix\"\n",
    "# Récupérer les attributs existants de cell_ids\n",
    "attrs_to_keep = [\"level\", \"grid_name\"]\n",
    "# should also have nest\n",
    "emission[\"cell_ids\"].attrs = {\n",
    "    key: value\n",
    "    for key, value in emission[\"cell_ids\"].attrs.items()\n",
    "    if key in attrs_to_keep\n",
    "}\n",
    "\n",
    "print(emission[\"cell_ids\"].attrs)\n",
    "emission = emission.pipe(xdggs.decode)\n",
    "\n",
    "predictor_factory = curry(\n",
    "    Gaussian1DHealpix,\n",
    "    cell_ids=emission[\"cell_ids\"].data,\n",
    "    grid_info=emission.dggs.grid_info,\n",
    "    truncate=4.0,\n",
    "    weights_threshold=1e-8,\n",
    "    pad_kwargs={\"mode\": \"constant\", \"constant_value\": 0},\n",
    "    optimize_convolution=True,\n",
    ")\n",
    "\n",
    "estimator = EagerEstimator(sigma=None, predictor_factory=predictor_factory)\n",
    "optimizer = EagerBoundsSearch(\n",
    "    estimator,\n",
    "    (1e-4, emission.attrs[\"max_sigma\"]),\n",
    "    optimizer_kwargs={\"disp\": 3, \"xtol\": tolerance},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Fit the model parameter to the data\n",
    "optimized = optimizer.fit(emission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the optimized parameters\n",
    "params = optimized.to_dict()\n",
    "pd.DataFrame.from_dict(params, orient=\"index\").to_json(\n",
    "    f\"{target_root}/parameters.json\", storage_options=storage_options\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "del optimized, emission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "## 8. **Compute State Probabilities and Tracks:** Calculate the probability distribution of the species in question and compute the tracks.\n",
    "\n",
    "This step involves predicting state probabilities using the optimised parameter sigma computed in the last step together with normalized emission matrix.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and modules for data analysis.\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import hvplot.xarray\n",
    "from pangeo_fish.hmm.estimator import EagerEstimator\n",
    "from pangeo_fish.io import save_trajectories\n",
    "\n",
    "# Recreate the Estimator\n",
    "params = pd.read_json(\n",
    "    f\"{target_root}/parameters.json\", storage_options=storage_options\n",
    ").to_dict()[0] | {\"predictor_factory\": predictor_factory}\n",
    "params.pop(\"predictor\")\n",
    "optimized = EagerEstimator(**params)\n",
    "optimized.predictor_factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Load the Data\n",
    "emission = xr.open_dataset(\n",
    "    f\"{target_root}/combined_1D.zarr\",\n",
    "    engine=\"zarr\",\n",
    "    chunks=default_chunk_cells,\n",
    "    inline_array=True,\n",
    "    storage_options=storage_options,\n",
    ").compute()\n",
    "\n",
    "# Predict the State Probabilities\n",
    "\n",
    "states = optimized.predict_proba(emission)\n",
    "states = states.to_dataset().chunk(default_chunk_cells).persist()\n",
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the data and visualize the sum of probabilities\n",
    "plot = states.sum([\"cells\"]).hvplot() + states.count([\"cells\"]).hvplot()\n",
    "hvplot.save(plot, f\"{target_root}/states_count_1D.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Save probability distirbution, state matrix.\n",
    "states.chunk(default_chunk_cells).to_zarr(\n",
    "    f\"{target_root}/states_1D.zarr\",\n",
    "    mode=\"w\",\n",
    "    consolidated=True,\n",
    "    storage_options=storage_options,\n",
    ")\n",
    "print(f\"{target_root}/states_1D.zarr\")\n",
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# decode tracks\n",
    "\n",
    "trajectories = optimized.decode(\n",
    "    emission,\n",
    "    states.fillna(0),\n",
    "    mode=track_modes,\n",
    "    progress=False,\n",
    "    additional_quantities=additional_track_quantities,\n",
    ")\n",
    "trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trajectories.\n",
    "# Here we can chose format parquet for loading files from 'R'\n",
    "# or chose to  format 'geoparquet' for further analysis of tracks using\n",
    "# geopands.\n",
    "\n",
    "save_trajectories(trajectories, target_root, storage_options, format=\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "del optimized, emission, states, trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "## 9. **Visualization:** Visualize the results of the analysis for interpretation and insight.\n",
    "\n",
    "\n",
    "In this step, we visualize various aspects of the analysis results to gain insights and interpret the model outcomes. We plot the emission matrix, which represents the likelihood of observing a specific temperature difference given the model parameters and configurations. Additionally, we visualize the state probabilities, showing the likelihood of the system being in different states at each time step. We also plot each of the tracks of the tagged fish, displaying their movement patterns over time. Finally, we create a movie that combines the emission matrix and state probabilities to provide a comprehensive visualization of the analysis results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import holoviews as hv\n",
    "import hvplot.xarray\n",
    "import cmocean\n",
    "import xmovie\n",
    "from pangeo_fish import visualization\n",
    "from pangeo_fish.io import read_trajectories, save_html_hvplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "# load trajectories\n",
    "trajectories = read_trajectories(\n",
    "    track_modes, target_root, storage_options, format=\"parquet\"\n",
    ")\n",
    "print(trajectories)\n",
    "# Plot trajectoriesand plot.\n",
    "\n",
    "plots = [\n",
    "    traj.hvplot(\n",
    "        c=\"speed\",\n",
    "        tiles=\"CartoLight\",\n",
    "        title=traj.id,\n",
    "        cmap=\"cmo.speed\"\n",
    "        #                ,xlim=bbox['longitude'],        ylim=bbox['latitude']\n",
    "        ,\n",
    "        width=300,\n",
    "        height=300,\n",
    "    )\n",
    "    for traj in trajectories.trajectories\n",
    "]\n",
    "plot = hv.Layout(plots).cols(2)\n",
    "\n",
    "filepath = f\"{target_root}/trajectories.html\"\n",
    "save_html_hvplot(plot, filepath, storage_options)\n",
    "\n",
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "states_1D = xr.open_dataset(\n",
    "    \"A19124/states_1D.zarr\",\n",
    "    engine=\"zarr\",\n",
    "    chunks={},\n",
    "    inline_array=True,\n",
    "    storage_options=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set required attributes cleanly\n",
    "states_1D[\"cell_ids\"].attrs[\"grid_name\"] = \"healpix\"\n",
    "# emission_1D[\"cell_ids\"].attrs[\"nest\"] = True\n",
    "# Récupérer les attributs existants de cell_ids\n",
    "attrs_to_keep = [\"level\", \"grid_name\"]\n",
    "# should also have nest\n",
    "states_1D[\"cell_ids\"].attrs = {\n",
    "    key: value\n",
    "    for key, value in states_1D[\"cell_ids\"].attrs.items()\n",
    "    if key in attrs_to_keep\n",
    "}\n",
    "states_1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "states_1D.pipe(xdggs.decode).isel(time=-1).compute().states.dggs.explore(alpha=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# load files for plotting\n",
    "\n",
    "emission = (\n",
    "    xr.open_dataset(\n",
    "        f\"{target_root}/emission.zarr\",\n",
    "        engine=\"zarr\",\n",
    "        chunks={},\n",
    "        inline_array=True,\n",
    "        storage_options=storage_options,\n",
    "    )\n",
    "    .rename_vars({\"pdf\": \"emission\"})\n",
    "    .drop_vars([\"final\", \"initial\"])\n",
    ")  # .where(emission[\"mask\"])\n",
    "states = xr.open_dataset(\n",
    "    f\"{target_root}/states_cells.zarr\",\n",
    "    engine=\"zarr\",\n",
    "    chunks={},\n",
    "    inline_array=True,\n",
    "    storage_options=storage_options,\n",
    ").where(emission[\"mask\"])\n",
    "data = xr.merge([states, emission.drop_vars([\"mask\"])])\n",
    "\n",
    "# visualize states and emission matrix.  Save the visualisation in an html file.\n",
    "#\n",
    "plot1 = visualization.plot_map(data[\"states\"], bbox)\n",
    "plot2 = visualization.plot_map(data[\"emission\"], bbox)\n",
    "plot = hv.Layout([plot1, plot2]).cols(2)\n",
    "filepath = f\"{target_root}/states_emission.html\"\n",
    "\n",
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Create Movies\n",
    "#\n",
    "mov = xmovie.Movie(\n",
    "    (\n",
    "        data.isel(time=slice(0, data.time.size - 1, time_step))\n",
    "        .chunk({\"time\": 1, \"x\": -1, \"y\": -1})\n",
    "        .pipe(lambda ds: ds.merge(ds[[\"longitude\", \"latitude\"]].compute()))\n",
    "    ).pipe(visualization.filter_by_states),\n",
    "    plotfunc=visualization.create_frame,\n",
    "    input_check=False,\n",
    "    pixelwidth=15 * 400,\n",
    "    pixelheight=12 * 400,\n",
    "    dpi=400,\n",
    ")\n",
    "## workaround dueto https://github.com/jbusecke/xmovie/issues/162\n",
    "# use local file system\n",
    "\n",
    "if target_root.startswith(\"s3://\"):\n",
    "    !mkdir -p movie\n",
    "    mov.save(\n",
    "        f\"./movie/states.mp4\",\n",
    "        overwrite_existing=True,\n",
    "    )\n",
    "    import s3fs\n",
    "\n",
    "    s3 = s3fs.S3FileSystem(**storage_options)\n",
    "    s3.put_file(f\"./movie/states.mp4\", f\"{target_root}/states.mp4\")\n",
    "else:\n",
    "    mov.save(\n",
    "        f\"{target_root}/states.mp4\",\n",
    "        overwrite_existing=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
